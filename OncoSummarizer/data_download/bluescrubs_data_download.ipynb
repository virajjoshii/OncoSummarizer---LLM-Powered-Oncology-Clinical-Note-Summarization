{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\"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\")\n",
        "\n",
        "# Load with streaming\n",
        "dataset = load_dataset(\"TheBlueScrubs/TheBlueScrubs-v1\", split=\"train\", streaming=True)\n",
        "\n",
        "# Shuffle the stream before sampling\n",
        "dataset = dataset.shuffle(seed=42, buffer_size=10000)\n",
        "\n",
        "# Collect 50k balanced-ish samples\n",
        "samples = []\n",
        "for i, example in enumerate(dataset):\n",
        "    if i >= 50000:\n",
        "        break\n",
        "    samples.append(example)\n",
        "\n",
        "print(\"✅ Collected\", len(samples), \"examples\")\n",
        "print(samples[0])\n"
      ],
      "metadata": {
        "id": "qOCJbV1ieJPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from google.colab import files\n",
        "\n",
        "# Shuffle for better splits\n",
        "random.shuffle(samples)\n",
        "\n",
        "# Train/val/test split\n",
        "train_size = int(0.8 * len(samples))\n",
        "val_size = int(0.1 * len(samples))\n",
        "\n",
        "train_data = samples[:train_size]\n",
        "val_data = samples[train_size:train_size+val_size]\n",
        "test_data = samples[train_size+val_size:]\n",
        "\n",
        "# Save as JSONL\n",
        "def save_json(filename, data):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        for ex in data:\n",
        "            json.dump(ex, f)\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "# Save with better naming\n",
        "save_json(\"bluescrubs_train.jsonl\", train_data)\n",
        "save_json(\"bluescrubs_val.jsonl\", val_data)\n",
        "save_json(\"bluescrubs_test.jsonl\", test_data)\n",
        "\n",
        "print(\"✅ Saved splits:\",\n",
        "      len(train_data), \"train /\",\n",
        "      len(val_data), \"val /\",\n",
        "      len(test_data), \"test\")\n",
        "\n",
        "# Download files\n",
        "files.download(\"bluescrubs_train.jsonl\")\n",
        "files.download(\"bluescrubs_val.jsonl\")\n",
        "files.download(\"bluescrubs_test.jsonl\")\n"
      ],
      "metadata": {
        "id": "xhIJEt7afKm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "import statistics\n",
        "\n",
        "# Pick which file to test (train/val/test)\n",
        "file_path = \"bluescrubs_train.jsonl\"   # or \"bluescrubs_val.jsonl\" / \"bluescrubs_test.jsonl\"\n",
        "\n",
        "# Load the JSONL file\n",
        "records = []\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        records.append(json.loads(line))\n",
        "\n",
        "print(f\"✅ Loaded {len(records)} records from {file_path}\")\n",
        "\n",
        "# --- Check unique sources ---\n",
        "sources = [ex.get(\"meta\", {}).get(\"source\", \"unknown\") for ex in records]\n",
        "print(\"Top sources:\", Counter(sources).most_common(10))\n",
        "\n",
        "# --- Check probability distribution ---\n",
        "probs = [float(ex.get(\"meta\", {}).get(\"probability\", 0)) for ex in records]\n",
        "print(\"Mean:\", statistics.mean(probs), \"Min:\", min(probs), \"Max:\", max(probs))\n"
      ],
      "metadata": {
        "id": "0cSF-usaiHWY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}