{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDVc9_hV5ObL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ======================\n",
        "# CONFIG\n",
        "# ======================\n",
        "FILES = {\n",
        "    \"rond\": {\n",
        "        \"train\": \"./ROND_train.csv\",\n",
        "        \"val\": \"./ROND_val.csv\",\n",
        "        \"test\": \"./ROND_test.csv\"\n",
        "    },\n",
        "    \"bluescrubs\": {\n",
        "        \"train\": \"./bluescrubs_train_clean.csv\",\n",
        "        \"val\": \"./bluescrubs_val_clean.csv\",\n",
        "        \"test\": \"./bluescrubs_test_clean.csv\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Tokenizers for BERT and Longformer\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "longformer_tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "\n",
        "# ======================\n",
        "# FUNCTION: Truncate + Chunk\n",
        "# ======================\n",
        "def truncate_and_chunk(text, tokenizer, max_tokens=512, stride=50):\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(tokens), max_tokens - stride):\n",
        "        chunk_tokens = tokens[i:i + max_tokens]\n",
        "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# ======================\n",
        "# PIPELINE for BlueScrubs (classification)\n",
        "# ======================\n",
        "def preprocess_bluescrubs(file_path, tokenizer, max_tokens, name=\"\"):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    new_texts = []\n",
        "    new_labels = []\n",
        "    chunk_counts = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        text = str(row[\"input\"])      # text column\n",
        "        label = row[\"output\"]         # classification label (0/1)\n",
        "\n",
        "        chunks = truncate_and_chunk(text, tokenizer, max_tokens=max_tokens)\n",
        "        chunk_counts.append(len(chunks))\n",
        "\n",
        "        for chunk in chunks:\n",
        "            new_texts.append(chunk)\n",
        "            new_labels.append(label)\n",
        "\n",
        "    new_df = pd.DataFrame({\"text\": new_texts, \"label\": new_labels})\n",
        "\n",
        "    # Validation\n",
        "    print(f\"\\nðŸ“Š Validation for {name}:\")\n",
        "    print(f\"  Total original docs: {len(df)}\")\n",
        "    print(f\"  Total new docs (after chunking): {len(new_df)}\")\n",
        "    print(f\"  Avg chunks per doc: {sum(chunk_counts)/len(chunk_counts):.2f}\")\n",
        "    print(f\"  Max chunks for a single doc: {max(chunk_counts)}\")\n",
        "\n",
        "    plt.hist(chunk_counts, bins=50, color='skyblue', edgecolor='black')\n",
        "    plt.title(f\"Chunk distribution per document ({name})\")\n",
        "    plt.xlabel(\"Chunks per document\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "    return new_df\n",
        "\n",
        "\n",
        "# ======================\n",
        "# ROND (summarization) - No chunking needed\n",
        "# ======================\n",
        "def load_rond(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df[[\"input\", \"output\"]]   # just keep text + summary\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Run preprocessing\n",
        "# ======================\n",
        "\n",
        "# ROND (no chunking)\n",
        "rond_train = load_rond(FILES[\"rond\"][\"train\"])\n",
        "rond_val   = load_rond(FILES[\"rond\"][\"val\"])\n",
        "rond_test  = load_rond(FILES[\"rond\"][\"test\"])\n",
        "\n",
        "# BlueScrubs (chunking for BERT + Longformer)\n",
        "bluescrubs_train_bert = preprocess_bluescrubs(FILES[\"bluescrubs\"][\"train\"], bert_tokenizer, 512, \"BERT - Train\")\n",
        "bluescrubs_val_bert   = preprocess_bluescrubs(FILES[\"bluescrubs\"][\"val\"], bert_tokenizer, 512, \"BERT - Val\")\n",
        "bluescrubs_test_bert  = preprocess_bluescrubs(FILES[\"bluescrubs\"][\"test\"], bert_tokenizer, 512, \"BERT - Test\")\n",
        "\n",
        "bluescrubs_train_long = preprocess_bluescrubs(FILES[\"bluescrubs\"][\"train\"], longformer_tokenizer, 4096, \"Longformer - Train\")\n",
        "bluescrubs_val_long   = preprocess_bluescrubs(FILES[\"bluescrubs\"][\"val\"], longformer_tokenizer, 4096, \"Longformer - Val\")\n",
        "bluescrubs_test_long  = preprocess_bluescrubs(FILES[\"bluescrubs\"][\"test\"], longformer_tokenizer, 4096, \"Longformer - Test\")\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Save outputs\n",
        "# ======================\n",
        "# ROND\n",
        "rond_train.to_csv(\"./rond_train_processed.csv\", index=False)\n",
        "rond_val.to_csv(\"./rond_val_processed.csv\", index=False)\n",
        "rond_test.to_csv(\"./rond_test_processed.csv\", index=False)\n",
        "\n",
        "# BlueScrubs\n",
        "bluescrubs_train_bert.to_csv(\"./bluescrubs_train_chunked_bert.csv\", index=False)\n",
        "bluescrubs_val_bert.to_csv(\"./bluescrubs_val_chunked_bert.csv\", index=False)\n",
        "bluescrubs_test_bert.to_csv(\"./bluescrubs_test_chunked_bert.csv\", index=False)\n",
        "\n",
        "bluescrubs_train_long.to_csv(\"./bluescrubs_train_chunked_longformer.csv\", index=False)\n",
        "bluescrubs_val_long.to_csv(\"./bluescrubs_val_chunked_longformer.csv\", index=False)\n",
        "bluescrubs_test_long.to_csv(\"./bluescrubs_test_chunked_longformer.csv\", index=False)\n"
      ]
    }
  ]
}