{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os; os.kill(os.getpid(), 9) #restarting the session\n"
      ],
      "metadata": {
        "id": "Mwnfmt8-J0xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers datasets accelerate evaluate rouge-score sentencepiece"
      ],
      "metadata": {
        "id": "oQbgvfdwaZkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "tWHDN0NEHuHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "HvpZ4hn6IbMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate datasets evaluate rouge-score sentencepiece\n"
      ],
      "metadata": {
        "id": "FIFQRKcUI26E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers accelerate datasets evaluate rouge-score sentencepiece\n",
        "!pip install \"transformers==4.46.2\" \"accelerate==1.2.1\" datasets evaluate rouge-score sentencepiece\n",
        "import os; os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "FdkR03_aLESY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Disable W&B logging globally ====\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# ==== Imports ====\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import torch\n",
        "\n",
        "# ==== Paths (ROND: no chunking) ====\n",
        "TRAIN = \"./rond_train_processed.csv\"\n",
        "VAL   = \"./rond_val_processed.csv\"\n",
        "TEST  = \"./rond_test_processed.csv\"\n",
        "\n",
        "# ==== Load CSVs ‚Üí HuggingFace Datasets ====\n",
        "def load_split(path):\n",
        "    df = pd.read_csv(path)\n",
        "    if \"instruction\" in df.columns:\n",
        "        df[\"source\"] = \"Instruction: \" + df[\"instruction\"].astype(str) + \"\\nInput: \" + df[\"input\"].astype(str)\n",
        "    else:\n",
        "        df[\"source\"] = df[\"input\"].astype(str)\n",
        "    df = df[[\"source\", \"output\"]].rename(columns={\"output\": \"target\"})\n",
        "    return Dataset.from_pandas(df)\n",
        "\n",
        "ds = DatasetDict({\n",
        "    \"train\": load_split(TRAIN),\n",
        "    \"validation\": load_split(VAL),\n",
        "    \"test\": load_split(TEST)\n",
        "})\n",
        "\n",
        "# ==== Model & Tokenizer ====\n",
        "MODEL_NAME = \"facebook/bart-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "max_source_len = 512\n",
        "max_target_len = 128\n",
        "\n",
        "def preprocess(batch):\n",
        "    model_in = tokenizer(batch[\"source\"], max_length=max_source_len, truncation=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(batch[\"target\"], max_length=max_target_len, truncation=True)\n",
        "    model_in[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_in\n",
        "\n",
        "tokenized = ds.map(preprocess, batched=True, remove_columns=ds[\"train\"].column_names)\n",
        "collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# ==== Metrics (ROUGE-L) ====\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "\n",
        "    # üß† Handle tuple outputs and logits ‚Üí token IDs\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    if isinstance(preds, np.ndarray) and preds.ndim == 3:\n",
        "        preds = np.argmax(preds, axis=-1)  # convert logits ‚Üí token IDs\n",
        "\n",
        "    # Convert torch tensors ‚Üí numpy\n",
        "    if isinstance(preds, torch.Tensor):\n",
        "        preds = preds.cpu().numpy()\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "    # Replace -100 (ignored tokens) with pad_token_id\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    # Decode\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute ROUGE\n",
        "    scores = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    return {\"rougeL\": scores[\"rougeL\"]}\n",
        "\n",
        "# ==== TrainingArguments ====\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./bart_rond_baseline\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    report_to=[],  # Disable wandb/tensorboard\n",
        "    disable_tqdm=False,\n",
        ")\n",
        "\n",
        "# ==== Trainer ====\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ==== Train ====\n",
        "print(\"üöÄ Training started...\\n\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# ==== Evaluate on test set ====\n",
        "print(\"\\nüîç Evaluating on test set...\\n\")\n",
        "test_metrics = trainer.evaluate(tokenized[\"test\"], metric_key_prefix=\"test\")\n",
        "print(\"‚úÖ Test set metrics:\", test_metrics)\n",
        "\n",
        "# ==== Visualize Training Progress ====\n",
        "if hasattr(trainer, \"state\") and trainer.state.log_history:\n",
        "    train_logs = [x for x in trainer.state.log_history if \"loss\" in x]\n",
        "    if train_logs:\n",
        "        steps = [x.get(\"step\", i) for i, x in enumerate(train_logs)]\n",
        "        losses = [x[\"loss\"] for x in train_logs]\n",
        "\n",
        "        plt.figure(figsize=(7, 4))\n",
        "        plt.plot(steps, losses, marker='o', label=\"Training Loss\")\n",
        "        plt.title(\"Training Progress\")\n",
        "        plt.xlabel(\"Steps\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "BHVKd8jbs4WT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}