{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate datasets --quiet\n",
        "\n",
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "metadata": {
        "id": "n0LeUoI7eLoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "u2tADps7bqlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcG5M-kMVl04"
      },
      "outputs": [],
      "source": [
        "# ==== Install deps (safe to run multiple times) ====\n",
        "!pip install -U transformers datasets accelerate evaluate scikit-learn matplotlib --quiet\n",
        "\n",
        "# ==== Imports ====\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "# ==== Disable external logging (W&B etc.) ====\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
        "\n",
        "# ==== Paths (chunked BlueScrubs for BERT) ====\n",
        "TRAIN = \"./bluescrubs_train_chunked_bert.csv\"\n",
        "VAL   = \"./bluescrubs_val_chunked_bert.csv\"\n",
        "TEST  = \"./bluescrubs_test_chunked_bert.csv\"\n",
        "\n",
        "# ==== Load CSVs ‚Üí Hugging Face Datasets ====\n",
        "def to_hfds(path):\n",
        "    df = pd.read_csv(path)\n",
        "    df[\"labels\"] = df[\"label\"].astype(int)   # Trainer expects 'labels'\n",
        "    return Dataset.from_pandas(df[[\"text\", \"labels\"]])\n",
        "\n",
        "ds = DatasetDict({\n",
        "    \"train\": to_hfds(TRAIN),\n",
        "    \"validation\": to_hfds(VAL),\n",
        "    \"test\": to_hfds(TEST)\n",
        "})\n",
        "\n",
        "# ==== Tokenizer & Preprocessing ====\n",
        "MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def preprocess(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, max_length=512)\n",
        "\n",
        "tokenized = ds.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# ==== Model ====\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "# ==== Metrics ====\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\":  accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"precision\": precision.compute(predictions=preds, references=labels, average=\"binary\")[\"precision\"],\n",
        "        \"recall\":    recall.compute(predictions=preds, references=labels, average=\"binary\")[\"recall\"],\n",
        "        \"f1\":        f1.compute(predictions=preds, references=labels, average=\"binary\")[\"f1\"],\n",
        "    }\n",
        "\n",
        "# ==== Fine-tuning Training Arguments (no evaluation_strategy) ====\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./bioclinicalbert_bluescrubs_finetuned\",\n",
        "\n",
        "    # üîß Fine-tuning changes vs baseline\n",
        "    learning_rate=1e-5,             # smaller LR than baseline (2e-5)\n",
        "    num_train_epochs=4,             # more epochs than baseline (2)\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=2,  # effective batch size 16\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "\n",
        "    # üìù Logging & checkpoints (simple, version-safe)\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    report_to=[],                   # no W&B / TB / MLflow\n",
        ")\n",
        "\n",
        "# ==== Trainer ====\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ==== Train (fine-tuning) ====\n",
        "trainer.train()\n",
        "\n",
        "# ==== Evaluate on Validation & Test Sets ====\n",
        "print(\"\\n===== Validation Results (Fine-tuned BioClinicalBERT) =====\")\n",
        "val_results = trainer.evaluate(tokenized[\"validation\"])\n",
        "for k, v in val_results.items():\n",
        "    try:\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "    except TypeError:\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "print(\"\\n===== Test Results (Fine-tuned BioClinicalBERT) =====\")\n",
        "test_results = trainer.evaluate(tokenized[\"test\"], metric_key_prefix=\"test\")\n",
        "for k, v in test_results.items():\n",
        "    try:\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "    except TypeError:\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "# ==== Training Logs & Plots (if available) ====\n",
        "logs = pd.DataFrame(trainer.state.log_history)\n",
        "print(\"\\n===== Log History (tail) =====\")\n",
        "print(logs.tail())\n",
        "\n",
        "# Plot Loss if possible\n",
        "if \"loss\" in logs.columns:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    x = logs[\"step\"] if \"step\" in logs.columns else range(len(logs[\"loss\"]))\n",
        "    plt.plot(x, logs[\"loss\"], label=\"Training Loss\", marker=\"o\")\n",
        "    if \"eval_loss\" in logs.columns:\n",
        "        plt.plot(x, logs[\"eval_loss\"], label=\"Validation Loss\", marker=\"o\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training vs Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot Accuracy and F1 if available\n",
        "if \"eval_accuracy\" in logs.columns and \"eval_f1\" in logs.columns:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    x = logs[\"step\"] if \"step\" in logs.columns else range(len(logs[\"eval_accuracy\"]))\n",
        "    plt.plot(x, logs[\"eval_accuracy\"], label=\"Validation Accuracy\", marker=\"o\")\n",
        "    plt.plot(x, logs[\"eval_f1\"], label=\"Validation F1\", marker=\"o\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(\"Validation Accuracy and F1 Score\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    }
  ]
}