{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \\\n",
        "  torch \\\n",
        "  transformers \\\n",
        "  datasets \\\n",
        "  accelerate \\\n",
        "  evaluate \\\n",
        "  scikit-learn \\\n",
        "  numpy \\\n",
        "  pandas\n"
      ],
      "metadata": {
        "id": "tMc8L5vooN8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRWnaUJifzdW"
      },
      "outputs": [],
      "source": [
        "# ==== Setup (clean version)\n",
        "!pip -q install transformers datasets accelerate evaluate scikit-learn matplotlib\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "# ==== Disable W&B and other external loggers\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"        # Disable W&B\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\" # Optional cleaner logs\n",
        "\n",
        "# ==== Paths (chunked for BERT)\n",
        "TRAIN = \"./bluescrubs_train_chunked_bert.csv\"\n",
        "VAL   = \"./bluescrubs_val_chunked_bert.csv\"\n",
        "TEST  = \"./bluescrubs_test_chunked_bert.csv\"\n",
        "\n",
        "# ==== Load data and convert to Hugging Face Datasets\n",
        "def to_hfds(path):\n",
        "    df = pd.read_csv(path)\n",
        "    df[\"labels\"] = df[\"label\"].astype(int)   # âœ… rename for Trainer compatibility\n",
        "    return Dataset.from_pandas(df[[\"text\", \"labels\"]])\n",
        "\n",
        "ds = DatasetDict({\n",
        "    \"train\": to_hfds(TRAIN),\n",
        "    \"validation\": to_hfds(VAL),\n",
        "    \"test\": to_hfds(TEST)\n",
        "})\n",
        "\n",
        "# ==== Tokenizer and Preprocessing\n",
        "MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def preprocess(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, max_length=512)\n",
        "\n",
        "# âœ… Keep 'labels' column; remove only 'text'\n",
        "tokenized = ds.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# ==== Model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "# ==== Metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\":  accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"precision\": precision.compute(predictions=preds, references=labels, average=\"binary\")[\"precision\"],\n",
        "        \"recall\":    recall.compute(predictions=preds, references=labels, average=\"binary\")[\"recall\"],\n",
        "        \"f1\":        f1.compute(predictions=preds, references=labels, average=\"binary\")[\"f1\"],\n",
        "    }\n",
        "\n",
        "# ==== Training Arguments (No W&B)\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./bioclinicalbert_bluescrubs_baseline\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    logging_strategy=\"epoch\",     # Log at each epoch\n",
        "    logging_dir=\"./logs\",         # Local logs only\n",
        "    report_to=[],                 # ðŸš« disables W&B/TensorBoard/MLflow\n",
        "    load_best_model_at_end=True,  # optional: load best model by eval metric\n",
        ")\n",
        "\n",
        "# ==== Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ==== Train\n",
        "trainer.train()\n",
        "\n",
        "# ==== Evaluate on Test Set\n",
        "results = trainer.evaluate(tokenized[\"test\"], metric_key_prefix=\"test\")\n",
        "print(\"\\n===== Test Results =====\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "# ==== Visualization inside Colab\n",
        "logs = pd.DataFrame(trainer.state.log_history)\n",
        "display(logs.tail())\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(logs[\"epoch\"], logs[\"loss\"], label=\"Training Loss\", marker=\"o\")\n",
        "if \"eval_loss\" in logs.columns:\n",
        "    plt.plot(logs[\"epoch\"], logs[\"eval_loss\"], label=\"Validation Loss\", marker=\"o\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Accuracy and F1 if available\n",
        "if \"eval_accuracy\" in logs.columns and \"eval_f1\" in logs.columns:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(logs[\"epoch\"], logs[\"eval_accuracy\"], label=\"Validation Accuracy\", marker=\"o\")\n",
        "    plt.plot(logs[\"epoch\"], logs[\"eval_f1\"], label=\"Validation F1\", marker=\"o\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(\"Validation Accuracy and F1 Score per Epoch\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers==4.45.0\n"
      ],
      "metadata": {
        "id": "UFYLnXSFT0CT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}