{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N25lFFlZ1rCt"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "\n",
        "# This will not show your key on screen\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "print(\"API key set! You can now run the Streamlit app cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import torch\n",
        "import streamlit as st\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        ")\n",
        "from openai import OpenAI\n",
        "\n",
        "# -------------------\n",
        "# Basic config\n",
        "# -------------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "LONGFORMER_MODEL_NAME = \"VirajJoshi/longformer-oncosummarizer-tuned\"\n",
        "BIOCLINICALBERT_MODEL_NAME = \"VirajJoshi/bioclinicalbert-oncosummarizer-tuned\"\n",
        "BART_MODEL_NAME = \"VirajJoshi/bart-rond-baseline-oncosummarizer\"\n",
        "\n",
        "ONCO_THRESHOLD = 0.5           # probability threshold for \"oncology-relevant\"\n",
        "MAX_BERT_CHUNKS = 4            # how many top chunks to keep for summarization\n",
        "BART_MAX_INPUT_TOKENS = 512\n",
        "BART_MAX_NEW_TOKENS = 128\n",
        "\n",
        "\n",
        "# -------------------\n",
        "# Model loading\n",
        "# -------------------\n",
        "\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    # Longformer doc classifier\n",
        "    long_tok = AutoTokenizer.from_pretrained(LONGFORMER_MODEL_NAME)\n",
        "    long_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        LONGFORMER_MODEL_NAME\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # BioClinicalBERT chunk classifier\n",
        "    bert_tok = AutoTokenizer.from_pretrained(BIOCLINICALBERT_MODEL_NAME)\n",
        "    bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        BIOCLINICALBERT_MODEL_NAME\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # BART summarizer\n",
        "    bart_tok = AutoTokenizer.from_pretrained(BART_MODEL_NAME)\n",
        "    bart_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        BART_MODEL_NAME\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    return long_tok, long_model, bert_tok, bert_model, bart_tok, bart_model\n",
        "\n",
        "\n",
        "# -------------------\n",
        "# Helper functions\n",
        "# -------------------\n",
        "\n",
        "def classify_document(text, long_tok, long_model):\n",
        "    \"\"\"Longformer doc-level oncology vs non-oncology.\"\"\"\n",
        "    inputs = long_tok(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=4096,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = long_model(**inputs).logits\n",
        "        probs = torch.softmax(logits, dim=-1)[0]\n",
        "\n",
        "    prob_non_onco = float(probs[0])\n",
        "    prob_onco = float(probs[1])\n",
        "    label = int(torch.argmax(probs))  # 0 = non-onco, 1 = onco\n",
        "\n",
        "    return label, prob_onco, prob_non_onco\n",
        "\n",
        "\n",
        "def select_relevant_chunks(text, bert_tok, bert_model,\n",
        "                           max_chunks=MAX_BERT_CHUNKS):\n",
        "    \"\"\"\n",
        "    Use BioClinicalBERT to score overlapping chunks and keep the top K\n",
        "    oncology-relevant ones. We then concatenate those chunks as input to BART.\n",
        "    \"\"\"\n",
        "    enc = bert_tok(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "    )\n",
        "\n",
        "    input_ids_list = enc[\"input_ids\"]  # (num_chunks, seq_len)\n",
        "\n",
        "    chunk_scores = []\n",
        "    chunk_texts = []\n",
        "\n",
        "    for i in range(input_ids_list.size(0)):\n",
        "        chunk_ids = input_ids_list[i].unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = bert_model(input_ids=chunk_ids).logits\n",
        "            probs = torch.softmax(logits, dim=-1)[0]\n",
        "            prob_onco = float(probs[1])\n",
        "\n",
        "        chunk_scores.append(prob_onco)\n",
        "        chunk_texts.append(\n",
        "            bert_tok.decode(input_ids_list[i], skip_special_tokens=True)\n",
        "        )\n",
        "\n",
        "    # Sort chunks by oncology probability (descending)\n",
        "    sorted_idx = sorted(\n",
        "        range(len(chunk_scores)),\n",
        "        key=lambda j: chunk_scores[j],\n",
        "        reverse=True,\n",
        "    )\n",
        "\n",
        "    selected_idx = sorted(sorted_idx[:max_chunks])  # keep original order\n",
        "\n",
        "    selected_chunks = [chunk_texts[j].strip() for j in selected_idx]\n",
        "    merged_text = \"\\n\\n\".join(selected_chunks)\n",
        "\n",
        "    return merged_text\n",
        "\n",
        "\n",
        "def summarize_with_bart(text, bart_tok, bart_model):\n",
        "    \"\"\"Generate abstractive summary using your tuned BART model.\"\"\"\n",
        "    inputs = bart_tok(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=BART_MAX_INPUT_TOKENS,\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        summary_ids = bart_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            num_beams=4,\n",
        "            max_new_tokens=BART_MAX_NEW_TOKENS,\n",
        "            min_new_tokens=30,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "\n",
        "    summary = bart_tok.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary.strip()\n",
        "\n",
        "\n",
        "def extract_json_with_gpt4(note_text, summary=None):\n",
        "    \"\"\"\n",
        "    Use GPT-4o-mini to turn the note/summary into structured JSON.\n",
        "    Requires OPENAI_API_KEY in env.\n",
        "    \"\"\"\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        st.error(\"OPENAI_API_KEY is not set. JSON mode unavailable.\")\n",
        "        return None\n",
        "\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are an assistant that extracts key oncology clinical information \"\n",
        "        \"from notes. You must respond ONLY with a single valid JSON object, \"\n",
        "        \"no prose. Use null for fields that are not mentioned.\\n\\n\"\n",
        "        \"JSON schema:\\n\"\n",
        "        \"{\\n\"\n",
        "        '  \\\"patient_age\\\": int or null,\\n'\n",
        "        '  \\\"patient_sex\\\": \\\"male\\\" | \\\"female\\\" | \\\"other\\\" | null,\\n'\n",
        "        '  \\\"primary_cancer_type\\\": string or null,\\n'\n",
        "        '  \\\"primary_site\\\": string or null,\\n'\n",
        "        '  \\\"stage\\\": string or null,\\n'\n",
        "        '  \\\"metastatic_sites\\\": [string] or [],\\n'\n",
        "        '  \\\"key_mutations\\\": [string] or [],\\n'\n",
        "        '  \\\"treatments\\\": [string] or [],\\n'\n",
        "        '  \\\"response_to_treatment\\\": string or null,\\n'\n",
        "        '  \\\"performance_status\\\": string or null,\\n'\n",
        "        '  \\\"important_findings\\\": [string] or [],\\n'\n",
        "        '  \\\"recommended_next_steps\\\": [string] or []\\n'\n",
        "        \"}\"\n",
        "    )\n",
        "\n",
        "    user_content = \"Original note:\\n\" + note_text\n",
        "    if summary is not None:\n",
        "        user_content += \"\\n\\nModel summary:\\n\" + summary\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "    return content\n",
        "\n",
        "\n",
        "def run_pipeline(note_text, mode, long_tok, long_model,\n",
        "                 bert_tok, bert_model,\n",
        "                 bart_tok, bart_model):\n",
        "    \"\"\"\n",
        "    mode = 'summary' or 'json'\n",
        "    Returns dict with outputs for Streamlit to display.\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "\n",
        "    # 1. Classify document with Longformer\n",
        "    label, prob_onco, prob_non = classify_document(\n",
        "        note_text, long_tok, long_model\n",
        "    )\n",
        "    result[\"longformer_label\"] = \"oncology\" if label == 1 else \"non-oncology\"\n",
        "    result[\"prob_onco\"] = prob_onco\n",
        "    result[\"prob_non_onco\"] = prob_non\n",
        "\n",
        "    # 2. Select relevant chunks using BioClinicalBERT\n",
        "    filtered_text = select_relevant_chunks(\n",
        "        note_text, bert_tok, bert_model\n",
        "    )\n",
        "    result[\"filtered_text\"] = filtered_text\n",
        "\n",
        "    # 3. Summarize with BART (always do it â€“ needed for both modes)\n",
        "    summary = summarize_with_bart(filtered_text, bart_tok, bart_model)\n",
        "    result[\"summary\"] = summary\n",
        "\n",
        "    # 4. If JSON requested, call GPT-4\n",
        "    if mode == \"json\":\n",
        "        json_str = extract_json_with_gpt4(note_text, summary=summary)\n",
        "        result[\"json\"] = json_str\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# -------------------\n",
        "# Streamlit UI\n",
        "# -------------------\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(\n",
        "        page_title=\"OncoSummarizer\",\n",
        "        layout=\"wide\",\n",
        "    )\n",
        "\n",
        "    st.title(\"OncoSummarizer ðŸ§¬\")\n",
        "    st.write(\n",
        "        \"Paste an oncology clinical note and choose whether you want a \"\n",
        "        \"concise **summary** or a **JSON structure of key clinical fields**.\"\n",
        "    )\n",
        "\n",
        "    # Load models once\n",
        "    (long_tok, long_model,\n",
        "     bert_tok, bert_model,\n",
        "     bart_tok, bart_model) = load_models()\n",
        "\n",
        "    st.markdown(\"**Click inside the box below and paste/type your clinical note:**\")\n",
        "\n",
        "    note_text = st.text_area(\n",
        "        \"Clinical note\",\n",
        "        value=\"\",\n",
        "        height=300,\n",
        "        key=\"note_input\",\n",
        "        placeholder=\"Paste the full clinical note here...\",\n",
        "    )\n",
        "\n",
        "    mode = st.radio(\n",
        "        \"What would you like to generate?\",\n",
        "        (\"Summary\", \"JSON with key elements\"),\n",
        "        index=0,\n",
        "        key=\"mode_radio\",\n",
        "    )\n",
        "\n",
        "    if st.button(\"Run OncoSummarizer\"):\n",
        "        if not note_text.strip():\n",
        "            st.warning(\"Please paste a clinical note.\")\n",
        "            return\n",
        "\n",
        "        with st.spinner(\"Running models... this might take a bit for long notes.\"):\n",
        "            mode_key = \"summary\" if mode == \"Summary\" else \"json\"\n",
        "            outputs = run_pipeline(\n",
        "                note_text,\n",
        "                mode_key,\n",
        "                long_tok, long_model,\n",
        "                bert_tok, bert_model,\n",
        "                bart_tok, bart_model,\n",
        "            )\n",
        "\n",
        "        # Show classification info\n",
        "        st.subheader(\"1. Document classification (Longformer)\")\n",
        "        st.write(f\"**Predicted label:** {outputs['longformer_label']}\")\n",
        "        st.write(\n",
        "            f\"Oncology prob: `{outputs['prob_onco']:.3f}`  |  \"\n",
        "            f\"Non-oncology prob: `{outputs['prob_non_onco']:.3f}`\"\n",
        "        )\n",
        "        if outputs[\"prob_onco\"] < ONCO_THRESHOLD:\n",
        "            st.warning(\n",
        "                \"This note is not strongly classified as oncology-related. \"\n",
        "                \"Results might be less meaningful.\"\n",
        "            )\n",
        "\n",
        "        # Show filtered text (optional, collapsible)\n",
        "        with st.expander(\"2. Filtered text used for summarization (BioClinicalBERT)\"):\n",
        "            st.write(outputs[\"filtered_text\"])\n",
        "            st.caption(\n",
        "                \"Chunks selected by BioClinicalBERT as most oncology-relevant.\"\n",
        "            )\n",
        "\n",
        "        # Show summary\n",
        "        st.subheader(\"3. Model summary (BART)\")\n",
        "        st.write(outputs[\"summary\"])\n",
        "\n",
        "        # Show JSON if requested\n",
        "        if mode_key == \"json\":\n",
        "            st.subheader(\"4. Structured JSON (GPT-4o-mini)\")\n",
        "            if outputs.get(\"json\") is None:\n",
        "                st.error(\n",
        "                    \"Could not generate JSON. Check OPENAI_API_KEY or logs.\"\n",
        "                )\n",
        "            else:\n",
        "                st.code(outputs[\"json\"], language=\"json\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "-ZvPGJ002Xvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit transformers torch openai\n",
        "!npm install -g localtunnel\n"
      ],
      "metadata": {
        "id": "88ZkaZwE31IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 6006 --server.headless true & npx localtunnel --port 6006\n"
      ],
      "metadata": {
        "id": "RgHS7HJP3_3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ifconfig.me\n"
      ],
      "metadata": {
        "id": "FOWmvVIH4swZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "id": "SdFkTzkFGJfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"Your Pyngrok Token\")\n"
      ],
      "metadata": {
        "id": "WLNuvQuWGLjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(6006)\n",
        "print(\"PUBLIC URL:\", public_url)\n",
        "\n",
        "!streamlit run app.py --server.port 6006 --server.headless true\n"
      ],
      "metadata": {
        "id": "q4MUoE6DGQ9q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}